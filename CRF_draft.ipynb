{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_features as ef\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is based on: https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_feat(infilepath):\n",
    "    \"\"\"\n",
    "    tbd\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(infilepath, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    ner_tags = list()\n",
    "    sentences = list()\n",
    "    \n",
    "    for line in text.split('\\n\\n'):\n",
    "        if line == '-DOCSTART- -X- O O' or '\"' in line:\n",
    "            continue\n",
    "        else:\n",
    "            sentence = list()\n",
    "            sent_ner_tags = list()\n",
    "                        \n",
    "            sent_items = line.split('\\n')\n",
    "            for idx, item in enumerate(sent_items):\n",
    "                if len(item.split(' ')) < 4:\n",
    "                    continue\n",
    "                else:\n",
    "                    feat_dict = dict()\n",
    "                                        \n",
    "                    token = item.split(' ')[0]\n",
    "                    ner_tag = ef.clean_ner_tags(item.split(' ')[3])\n",
    "                    lemma = ef.lemmatizer(token)\n",
    "                    if idx == 0 and token.istitle():\n",
    "                        shape = 'upcase_BOS'\n",
    "                    elif idx != 0 and token.istitle():\n",
    "                        shape = 'upcase_IN'\n",
    "                    elif token.islower():\n",
    "                        shape = 'lowcase'\n",
    "                    elif token.isupper():\n",
    "                        shape = 'all_caps'\n",
    "                    else:\n",
    "                        shape = 'other'\n",
    "                    \n",
    "                    feat_dict['lemma'] = lemma\n",
    "                    feat_dict['shape'] = shape\n",
    "                    \n",
    "                    sent_ner_tags.append(ner_tag)              \n",
    "                    sentence.append(feat_dict)\n",
    "                    \n",
    "            ner_tags.append(sent_ner_tags)\n",
    "            sentences.append(sentence)\n",
    "        \n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep train and test data\n",
    "\n",
    "train_sents, train_tags = crf_feat('data/train_reuters.en')\n",
    "\n",
    "test_sents, test_tags = crf_feat('data/test.conll')\n",
    "\n",
    "# remove empty lists:\n",
    "\n",
    "for idx, item in enumerate(train_sents):\n",
    "    if item == []:\n",
    "        train_sents.pop(idx)\n",
    "        train_tags.pop(idx)\n",
    "        \n",
    "for idx, item in enumerate(test_sents):\n",
    "    if item == []:\n",
    "        test_sents.pop(idx)\n",
    "        test_tags.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features; the format need to be as follows:\n",
    "# X - list of lists of dicts\n",
    "# y - list of lists of strings\n",
    "\n",
    "X_train = train_sents\n",
    "y_train = train_tags\n",
    "\n",
    "X_test = test_sents\n",
    "y_test = test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train CRF model\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1-score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# training F1-score\n",
    "\n",
    "y_pred_crf_train = crf.predict(X_train)\n",
    "\n",
    "print(f\"Training F1-score: {metrics.flat_f1_score(y_train, y_pred_crf_train, average='weighted').round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# testing F1-score\n",
    "\n",
    "y_pred_crf = crf.predict(X_test)\n",
    "\n",
    "print(f\"Test F1-score: {metrics.flat_f1_score(y_test, y_pred_crf, average='weighted').round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.930     0.835     0.880      1961\n",
      "        MISC      0.924     0.792     0.853      1111\n",
      "           O      0.988     0.993     0.990     34736\n",
      "         ORG      0.807     0.784     0.796      1868\n",
      "         PER      0.863     0.928     0.894      2810\n",
      "\n",
      "   micro avg      0.967     0.967     0.967     42486\n",
      "   macro avg      0.902     0.866     0.883     42486\n",
      "weighted avg      0.967     0.967     0.967     42486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# precision, recall, F1-score per label\n",
    "\n",
    "print(metrics.flat_classification_report(y_test, y_pred_crf, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "PER    -> PER     4.193654\n",
      "MISC   -> MISC    3.971928\n",
      "ORG    -> ORG     3.735996\n",
      "ORG    -> O       2.152950\n",
      "O      -> O       1.613371\n",
      "MISC   -> O       1.496939\n",
      "LOC    -> LOC     1.349455\n",
      "PER    -> O       1.241341\n",
      "LOC    -> O       1.120530\n",
      "O      -> PER     0.825543\n",
      "O      -> MISC    0.825199\n",
      "MISC   -> PER     0.333931\n",
      "O      -> LOC     -0.304484\n",
      "O      -> ORG     -0.590443\n",
      "ORG    -> MISC    -0.737057\n",
      "LOC    -> MISC    -0.942692\n",
      "ORG    -> PER     -1.333312\n",
      "MISC   -> ORG     -1.787679\n",
      "PER    -> MISC    -2.751714\n",
      "MISC   -> LOC     -2.808083\n",
      "\n",
      "Top unlikely transitions:\n",
      "MISC   -> O       1.496939\n",
      "LOC    -> LOC     1.349455\n",
      "PER    -> O       1.241341\n",
      "LOC    -> O       1.120530\n",
      "O      -> PER     0.825543\n",
      "O      -> MISC    0.825199\n",
      "MISC   -> PER     0.333931\n",
      "O      -> LOC     -0.304484\n",
      "O      -> ORG     -0.590443\n",
      "ORG    -> MISC    -0.737057\n",
      "LOC    -> MISC    -0.942692\n",
      "ORG    -> PER     -1.333312\n",
      "MISC   -> ORG     -1.787679\n",
      "PER    -> MISC    -2.751714\n",
      "MISC   -> LOC     -2.808083\n",
      "LOC    -> PER     -3.271771\n",
      "ORG    -> LOC     -3.622587\n",
      "LOC    -> ORG     -3.851031\n",
      "PER    -> LOC     -3.873774\n",
      "PER    -> ORG     -4.977887\n"
     ]
    }
   ],
   "source": [
    "# show the transitions that the classifier learned\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
